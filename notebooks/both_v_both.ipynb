{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burns both v smith both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,glob\n",
    "sys.path.append('/home/simon/git/burns/code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/'\n",
    "blank_mz_tol = 5\n",
    "mz_tol = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 9\n",
      "('/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS1a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS2a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS3a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS4a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS5a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS5a3.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS5a2.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS6a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS13a1.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS13a2.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS13a3.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS13a.mzML')\n",
      "('/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS10a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS11a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS11a3.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS11a2.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS11a1.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS14a.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS14a1.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS14a2.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/MSS14a3.mzML')\n",
      "['/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170601175639.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509173322.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170601181435.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509170513.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509160722.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170601173852.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509163024.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170601170305.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509170746.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170601164510.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509163257.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170509173047.mzML', '/home/simon/Dropbox/BioResearch/Meta_clustering/Burns/simon_test/Blank_170601172100.mzML']\n"
     ]
    }
   ],
   "source": [
    "group1 = ['MSS1','MSS2','MSS3','MSS4','MSS5','MSS6','MSS13']\n",
    "group2 = ['MSS10','MSS11','MSS14']\n",
    "files1 = []\n",
    "files2 = []\n",
    "for g in group1:\n",
    "    pattern = data_dir + g + 'a' + '*.mzML'\n",
    "    matches = glob.glob(pattern)\n",
    "    for match in matches:\n",
    "        end_bit = match.split('/')[-1]\n",
    "        bfile = end_bit.split('a')\n",
    "        bfilename =  \"\".join(bfile[:-1]) + 'b' + bfile[-1]\n",
    "        files1.append((match,data_dir+bfilename))\n",
    "for g in group2:\n",
    "    pattern = data_dir + g + 'a' + '*.mzML'\n",
    "    matches = glob.glob(pattern)\n",
    "    for match in matches:\n",
    "        end_bit = match.split('/')[-1]\n",
    "        bfile = end_bit.split('a')\n",
    "        bfilename =  \"\".join(bfile[:-1]) + 'b' + bfile[-1]\n",
    "        files2.append((match,data_dir + bfilename))\n",
    "\n",
    "group1,paper1 = zip(*files1)\n",
    "group2,paper2 = zip(*files2)\n",
    "\n",
    "print len(group1),len(group2)\n",
    "print group1\n",
    "print group2\n",
    "papers = paper1+paper2\n",
    "\n",
    "\n",
    "blank_patterns = 'Blank_*'\n",
    "blank_files = glob.glob(data_dir + blank_patterns)\n",
    "print blank_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group1 = group1 + paper1\n",
    "group2 = group2 + paper2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_di import load_di\n",
    "\n",
    "# s = load_di(group1[0])\n",
    "import glob\n",
    "data = {}\n",
    "for f in files1+files2:\n",
    "    out_file = '../csv_files/' + f[0].split('/')[-1].split('.')[0]+'.csv'\n",
    "#     data[f[0]] = load_di(f[0],blank_file = f[1],out_file = out_file,normalise = None,mz_tol = blank_mz_tol)\n",
    "    data[f[0]] = load_di(f[0],out_file = out_file,normalise = None,mz_tol = blank_mz_tol)\n",
    "    data[f[1]] = load_di(f[1],out_file = out_file,normalise = None,mz_tol = blank_mz_tol)\n",
    "\n",
    "for b in blank_files:\n",
    "    out_file = '../csv_files/' + b.split('/')[-1].split('.')[0]+'.csv'\n",
    "    data[b] = load_di(b,out_file = out_file,normalise = None,mz_tol = blank_mz_tol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write out a csv file for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22642 peaks\n",
      "22642 peaks\n",
      "22642 peaks\n",
      "22642 peaks\n",
      "22642 peaks\n",
      "22642 peaks\n",
      "22642 peaks\n",
      "22642 peaks\n"
     ]
    }
   ],
   "source": [
    "from matching import Greedy\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "for log in ['log','nolog']:\n",
    "    for missing in ['nan',0.01]:\n",
    "        for normalise in ['none','tus']:\n",
    "            options = {'log':log,'missing':missing,'normalise':normalise}\n",
    "            n_tests = 0\n",
    "\n",
    "            if options['normalise'] == 'tus':\n",
    "                normalised_data = {}\n",
    "                for f,spec in data.items():\n",
    "                    mz_list,i_list = zip(*spec)\n",
    "                    tot_intensity = sum(i_list)\n",
    "                    new_intensity = [1000.0*i/tot_intensity for i in i_list]\n",
    "                    new_spec = zip(mz_list,new_intensity)\n",
    "                    normalised_data[f] = new_spec\n",
    "            elif options['normalise'] == 'none':\n",
    "                normalised_data = data\n",
    "            \n",
    "            g = Greedy()\n",
    "            mp = g.process(normalised_data,mz_tol)\n",
    "            \n",
    "            \n",
    "#             new_mp = []\n",
    "#             # remove peaksets that only appear in blanks\n",
    "#             for ps in mp:\n",
    "#                 for p in ps.peaks:\n",
    "#                     if p[2] in group1 or p[2] in group2:\n",
    "#                         found = True\n",
    "#                 if found:\n",
    "#                     new_mp.append(ps)\n",
    "            \n",
    "#             mp = new_mp\n",
    "            \n",
    "            display_names = {}\n",
    "            for g in group1+group2:\n",
    "                display_names[g] = g.split('/')[-1]\n",
    "            import csv\n",
    "            all_names = list(group1+group2)\n",
    "            res = []\n",
    "\n",
    "\n",
    "            heads = [\"mz\"] + [display_names[a] for a in all_names] + [\"n_blank\"] + [\"pval\"] +[\"qval\"]\n",
    "            for ps in mp:\n",
    "                row = [ps.mean_mz]\n",
    "                g1 = []\n",
    "                g2 = []\n",
    "                for n in all_names:\n",
    "                    peak = filter(lambda x: x[2] == n,ps.peaks)\n",
    "                    if len(peak) == 0:\n",
    "                        row.append(\"nan\")\n",
    "                    else:\n",
    "                        # take max intensity\n",
    "                        peak = sorted(peak,key = lambda x: x[1], reverse = True)\n",
    "                        row.append(peak[0][1])\n",
    "                    if n in group1:\n",
    "                        g1.append(row[-1])\n",
    "                    if n in group2:\n",
    "                        g2.append(row[-1])\n",
    "                        \n",
    "                ng1 = sum([1 for a in g1 if not a == \"nan\"])\n",
    "                ng2 = sum([1 for a in g2 if not a == \"nan\"])\n",
    "                if options['missing'] == 'nan':\n",
    "                    g1 = [a for a in g1 if not a == \"nan\"]\n",
    "                    g2 = [a for a in g2 if not a == \"nan\"]\n",
    "                else:\n",
    "                    g1 = [a if not a == \"nan\" else options[\"missing\"] for a in g1]\n",
    "                    g2 = [a if not a == \"nan\" else options[\"missing\"] for a in g2]\n",
    "                if options['log'] == 'log':\n",
    "                    g1 = [np.log10(a) for a in g1]\n",
    "                    g2 = [np.log10(a) for a in g2]\n",
    "                \n",
    "                \n",
    "                \n",
    "                if ng1 > 1 or ng2 > 1:\n",
    "                    t,p = ttest_ind(g1,g2,equal_var=True)\n",
    "                    n_tests += 1\n",
    "                else:\n",
    "                    p = 999\n",
    "                    \n",
    "                b_count = 0\n",
    "                for n in blank_files:\n",
    "                    peak = filter(lambda x: x[2] == n,ps.peaks)\n",
    "                    if len(peak) > 0:\n",
    "                        b_count += 1\n",
    "                \n",
    "                row.append(b_count)\n",
    "                \n",
    "                row.append(p)\n",
    "                res.append(row)\n",
    "            \n",
    "            res = sorted(res,key = lambda x: x[-1])\n",
    "            # compute q-value\n",
    "            for i,r in enumerate(res):\n",
    "                if r[-1] < 999:\n",
    "                    q = r[-1]*n_tests / (1.0*(i+1))\n",
    "                    r.append(q)\n",
    "                else:\n",
    "                    r.append(\"nan\")\n",
    "                    \n",
    "            \n",
    "\n",
    "            with open(\"spreadsheets/both_v_both/both_v_both_{}_{}_{}.csv\".format(options['log'],options['missing'],options['normalise']),'w') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(heads)\n",
    "                for r in res:\n",
    "                    writer.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(blank_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp = sorted(mp,key = lambda x: x.mean_mz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[328.0816095899931]\n"
     ]
    }
   ],
   "source": [
    "print [a.mean_mz for a in mp if a.mean_mz > 328.08 and a.mean_mz < 328.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.892254147481\n"
     ]
    }
   ],
   "source": [
    "p1 = 2572\n",
    "p2 = 2573\n",
    "print 1e6*abs(mp[p1].mean_mz-mp[p2].mean_mz)/mp[p1].mean_mz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
